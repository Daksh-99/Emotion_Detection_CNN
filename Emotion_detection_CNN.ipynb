{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf6573e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf6573e7",
        "outputId": "b37a6b3b-3d25-41e0-95ea-e1511a279b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from keras_preprocessing) (2.0.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from keras_preprocessing) (1.17.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "# Importing additional libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Enabling inline plotting for Jupyter Notebooks\n",
        "%matplotlib inline\n",
        "!pip install keras_preprocessing\n",
        "# Importing necessary libraries from Keras\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import model_from_json\n",
        "from keras_preprocessing.image import load_img\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xwX7wzgpcjxm",
      "metadata": {
        "id": "xwX7wzgpcjxm"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905837b3",
      "metadata": {
        "id": "905837b3"
      },
      "outputs": [],
      "source": [
        "#Define directories for training and testing images\n",
        "TRAIN_DIR = '/content/drive/MyDrive/Colab Notebooks/Emotion-detection/images/train'\n",
        "TEST_DIR = '/content/drive/MyDrive/Colab Notebooks/Emotion-detection/images/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda524b2",
      "metadata": {
        "id": "eda524b2"
      },
      "outputs": [],
      "source": [
        "def createdataframe(dir):\n",
        "    # Initializing lists to store image paths and labels\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterating through each label directory\n",
        "    for label in os.listdir(dir):\n",
        "        # Iterating through each image in the label directory\n",
        "        for imagename in os.listdir(os.path.join(dir,label)):\n",
        "            # Appending the image path and corresponding label\n",
        "            image_paths.append(os.path.join(dir,label,imagename))\n",
        "            labels.append(label)\n",
        "        # Printing progress for each label\n",
        "        print(label, \"completed\")\n",
        "\n",
        "    # Returning the lists of image paths and labels\n",
        "    return image_paths, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2e2de11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2e2de11",
        "outputId": "a4e15038-a94f-4a19-8e42-50ea16cdce70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "disgust completed\n",
            "fear completed\n",
            "angry completed\n",
            "neutral completed\n",
            "sad completed\n",
            "happy completed\n",
            "surprise completed\n"
          ]
        }
      ],
      "source": [
        "# Creating an empty DataFrame\n",
        "train = pd.DataFrame()\n",
        "\n",
        "# Filing the DataFrame with image paths and labels using the createdataframe function\n",
        "train['image'], train['label'] = createdataframe(TRAIN_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e551d2b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e551d2b3",
        "outputId": "0c72930e-53ba-4177-ea28-fcec39ca1bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "surprise completed\n",
            "fear completed\n",
            "neutral completed\n",
            "sad completed\n",
            "disgust completed\n",
            "happy completed\n",
            "angry completed\n"
          ]
        }
      ],
      "source": [
        "# Creating an empty DataFrame\n",
        "test = pd.DataFrame()\n",
        "# Filing the DataFrame with image paths and labels using the createdataframe function\n",
        "test['image'], test['label'] = createdataframe(TEST_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c777cb0c",
      "metadata": {
        "id": "c777cb0c"
      },
      "outputs": [],
      "source": [
        "# Importing the tqdm library's notebook version\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a300029",
      "metadata": {
        "id": "5a300029"
      },
      "outputs": [],
      "source": [
        "def extract_features(images):\n",
        "    # Initializing an empty list to store features\n",
        "    features = []\n",
        "    # Looping through each image filename in the list, displaying a progress bar\n",
        "    for image in tqdm(images):\n",
        "        # Loading the image in grayscale\n",
        "        img = load_img(image, color_mode=\"grayscale\")\n",
        "        # Converting the image to a numpy array\n",
        "        img = np.array(img)\n",
        "        # Appending the image array to the features list\n",
        "        features.append(img)\n",
        "\n",
        "    # Converting the list of image arrays to a numpy array\n",
        "    features = np.array(features)\n",
        "    # Reshapeing the array to have dimensions (number of images, height, width, channels)\n",
        "    features = features.reshape(len(features), 48, 48, 1)\n",
        "    # Returning the extracted features\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1693abc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "32100594dfe04919bab48896b448acd4"
          ]
        },
        "id": "1693abc1",
        "outputId": "b1982644-d17b-40c1-e3af-1b1d81e4d05a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32100594dfe04919bab48896b448acd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/28710 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_features = extract_features(train['image'])\n",
        "test_features = extract_features(test['image'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e19434",
      "metadata": {
        "id": "64e19434"
      },
      "outputs": [],
      "source": [
        "# Divide the pixel values of the training features by 255.0 to scale them between 0 and 1\n",
        "x_train = train_features/255.0\n",
        "x_test = test_features/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "995fc837",
      "metadata": {
        "id": "995fc837"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4c6895",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "ca4c6895",
        "outputId": "90510c95-569d-4d62-e3e5-7970591f0144"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initializing a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "# Fit the LabelEncodeing to the labels in the 'label' column of the 'train' dataframe\n",
        "le.fit(train['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c59ea9e0",
      "metadata": {
        "id": "c59ea9e0"
      },
      "outputs": [],
      "source": [
        "# Transforming the categorical labels in the 'label' column of the training data using the fitted LabelEncoder\n",
        "y_train = le.transform(train['label'])\n",
        "# Transforming the categorical labels in the 'label' column of the test data using the fitted LabelEncoder\n",
        "y_test = le.transform(test['label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5313f6cb",
      "metadata": {
        "id": "5313f6cb"
      },
      "outputs": [],
      "source": [
        "# Converting the numerical labels for training data into one-hot encoded vectors with 7 classes\n",
        "y_train = to_categorical(y_train, num_classes=7)\n",
        "# Converting the numerical labels for test data into one-hot encoded vectors with 7 classes\n",
        "y_test = to_categorical(y_test, num_classes=7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ytxEEg1ca0b",
      "metadata": {
        "id": "6ytxEEg1ca0b"
      },
      "source": [
        "## Model initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c24678ea",
      "metadata": {
        "id": "c24678ea"
      },
      "outputs": [],
      "source": [
        "# Initializing a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "# Adding a convolutional layer with 128 filters, each with a 3x3 kernel size, ReLU activation function, and input shape of (48, 48, 1)\n",
        "model.add(Input(shape=(48, 48, 1)))  # Define the input shape explicitly\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))# Adding a max pooling layer with a pool size of 2x2\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# Adding a dropout layer with a dropout rate of 0.4\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Repeating the above pattern with different parameters for additional convolutional layers\n",
        "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Flatten the output of the convolutional layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layers\n",
        "# Adding a dense layer with 512 neurons and ReLU activation function\n",
        "model.add(Dense(512, activation='relu'))\n",
        "# Adding a dropout layer with a dropout rate of 0.4\n",
        "model.add(Dropout(0.4))\n",
        "# Adding a dense layer with 256 neurons and ReLU activation function\n",
        "model.add(Dense(256, activation='relu'))\n",
        "# Adding a dropout layer with a dropout rate of 0.3\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output layer\n",
        "# Adding a dense layer with 7 neurons (since there are 7 classes) and softmax activation function\n",
        "model.add(Dense(7, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f432b14",
      "metadata": {
        "id": "2f432b14"
      },
      "outputs": [],
      "source": [
        "# Compile the model with Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a03446",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7a03446",
        "outputId": "29ff3274-54ea-4226-a1b5-1410532fcbb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 98ms/step - accuracy: 0.2411 - loss: 1.8357 - val_accuracy: 0.2472 - val_loss: 1.8149\n",
            "Epoch 2/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.2495 - loss: 1.8091 - val_accuracy: 0.2553 - val_loss: 1.7836\n",
            "Epoch 3/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 46ms/step - accuracy: 0.2677 - loss: 1.7670 - val_accuracy: 0.3318 - val_loss: 1.6732\n",
            "Epoch 4/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.3342 - loss: 1.6670 - val_accuracy: 0.4277 - val_loss: 1.4767\n",
            "Epoch 5/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - accuracy: 0.4002 - loss: 1.5313 - val_accuracy: 0.4189 - val_loss: 1.4700\n",
            "Epoch 6/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.4300 - loss: 1.4655 - val_accuracy: 0.4919 - val_loss: 1.3196\n",
            "Epoch 7/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.4552 - loss: 1.4123 - val_accuracy: 0.5018 - val_loss: 1.2896\n",
            "Epoch 8/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.4740 - loss: 1.3666 - val_accuracy: 0.5176 - val_loss: 1.2525\n",
            "Epoch 9/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 49ms/step - accuracy: 0.4913 - loss: 1.3341 - val_accuracy: 0.5311 - val_loss: 1.2188\n",
            "Epoch 10/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.4967 - loss: 1.3087 - val_accuracy: 0.5337 - val_loss: 1.2167\n",
            "Epoch 11/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.5103 - loss: 1.2868 - val_accuracy: 0.5351 - val_loss: 1.2022\n",
            "Epoch 12/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.5127 - loss: 1.2808 - val_accuracy: 0.5521 - val_loss: 1.1784\n",
            "Epoch 13/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.5225 - loss: 1.2630 - val_accuracy: 0.5534 - val_loss: 1.1602\n",
            "Epoch 14/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.5272 - loss: 1.2415 - val_accuracy: 0.5567 - val_loss: 1.1448\n",
            "Epoch 15/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.5282 - loss: 1.2279 - val_accuracy: 0.5666 - val_loss: 1.1452\n",
            "Epoch 16/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - accuracy: 0.5371 - loss: 1.2262 - val_accuracy: 0.5642 - val_loss: 1.1284\n",
            "Epoch 17/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.5441 - loss: 1.2036 - val_accuracy: 0.5641 - val_loss: 1.1339\n",
            "Epoch 18/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.5524 - loss: 1.1835 - val_accuracy: 0.5702 - val_loss: 1.1206\n",
            "Epoch 19/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.5570 - loss: 1.1749 - val_accuracy: 0.5746 - val_loss: 1.1089\n",
            "Epoch 20/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.5572 - loss: 1.1656 - val_accuracy: 0.5722 - val_loss: 1.1119\n",
            "Epoch 21/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.5652 - loss: 1.1553 - val_accuracy: 0.5772 - val_loss: 1.1007\n",
            "Epoch 22/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.5696 - loss: 1.1442 - val_accuracy: 0.5762 - val_loss: 1.1200\n",
            "Epoch 23/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.5679 - loss: 1.1501 - val_accuracy: 0.5822 - val_loss: 1.0931\n",
            "Epoch 24/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.5702 - loss: 1.1364 - val_accuracy: 0.5956 - val_loss: 1.0782\n",
            "Epoch 25/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.5778 - loss: 1.1180 - val_accuracy: 0.5948 - val_loss: 1.0908\n",
            "Epoch 26/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.5748 - loss: 1.1156 - val_accuracy: 0.5948 - val_loss: 1.0706\n",
            "Epoch 27/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.5790 - loss: 1.1144 - val_accuracy: 0.5936 - val_loss: 1.0707\n",
            "Epoch 28/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 50ms/step - accuracy: 0.5842 - loss: 1.0914 - val_accuracy: 0.5928 - val_loss: 1.0724\n",
            "Epoch 29/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.5863 - loss: 1.0950 - val_accuracy: 0.5970 - val_loss: 1.0626\n",
            "Epoch 30/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.5896 - loss: 1.0906 - val_accuracy: 0.5935 - val_loss: 1.0653\n",
            "Epoch 31/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.5997 - loss: 1.0705 - val_accuracy: 0.6013 - val_loss: 1.0500\n",
            "Epoch 32/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.5964 - loss: 1.0575 - val_accuracy: 0.6008 - val_loss: 1.0570\n",
            "Epoch 33/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.5995 - loss: 1.0661 - val_accuracy: 0.5963 - val_loss: 1.0667\n",
            "Epoch 34/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6070 - loss: 1.0446 - val_accuracy: 0.6009 - val_loss: 1.0496\n",
            "Epoch 35/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.6043 - loss: 1.0408 - val_accuracy: 0.6030 - val_loss: 1.0535\n",
            "Epoch 36/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6077 - loss: 1.0351 - val_accuracy: 0.6026 - val_loss: 1.0532\n",
            "Epoch 37/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.6087 - loss: 1.0484 - val_accuracy: 0.5991 - val_loss: 1.0642\n",
            "Epoch 38/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 51ms/step - accuracy: 0.6098 - loss: 1.0305 - val_accuracy: 0.6074 - val_loss: 1.0410\n",
            "Epoch 39/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.6135 - loss: 1.0376 - val_accuracy: 0.6030 - val_loss: 1.0424\n",
            "Epoch 40/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.6213 - loss: 1.0124 - val_accuracy: 0.6087 - val_loss: 1.0347\n",
            "Epoch 41/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 50ms/step - accuracy: 0.6218 - loss: 1.0023 - val_accuracy: 0.6108 - val_loss: 1.0326\n",
            "Epoch 42/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 50ms/step - accuracy: 0.6283 - loss: 0.9969 - val_accuracy: 0.6105 - val_loss: 1.0411\n",
            "Epoch 43/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6233 - loss: 1.0043 - val_accuracy: 0.6132 - val_loss: 1.0302\n",
            "Epoch 44/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6177 - loss: 1.0089 - val_accuracy: 0.6130 - val_loss: 1.0285\n",
            "Epoch 45/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6260 - loss: 1.0009 - val_accuracy: 0.6134 - val_loss: 1.0382\n",
            "Epoch 46/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6304 - loss: 0.9876 - val_accuracy: 0.6165 - val_loss: 1.0195\n",
            "Epoch 47/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.6314 - loss: 0.9816 - val_accuracy: 0.6187 - val_loss: 1.0266\n",
            "Epoch 48/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.6390 - loss: 0.9606 - val_accuracy: 0.6198 - val_loss: 1.0209\n",
            "Epoch 49/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.6304 - loss: 0.9807 - val_accuracy: 0.6106 - val_loss: 1.0386\n",
            "Epoch 50/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.6399 - loss: 0.9620 - val_accuracy: 0.6197 - val_loss: 1.0200\n",
            "Epoch 51/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.6354 - loss: 0.9653 - val_accuracy: 0.6205 - val_loss: 1.0125\n",
            "Epoch 52/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 47ms/step - accuracy: 0.6507 - loss: 0.9493 - val_accuracy: 0.6210 - val_loss: 1.0229\n",
            "Epoch 53/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6454 - loss: 0.9477 - val_accuracy: 0.6224 - val_loss: 1.0221\n",
            "Epoch 54/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.6516 - loss: 0.9392 - val_accuracy: 0.6186 - val_loss: 1.0196\n",
            "Epoch 55/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 50ms/step - accuracy: 0.6545 - loss: 0.9241 - val_accuracy: 0.6253 - val_loss: 1.0132\n",
            "Epoch 56/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6556 - loss: 0.9298 - val_accuracy: 0.6221 - val_loss: 1.0143\n",
            "Epoch 57/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 48ms/step - accuracy: 0.6531 - loss: 0.9340 - val_accuracy: 0.6247 - val_loss: 1.0176\n",
            "Epoch 58/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 50ms/step - accuracy: 0.6590 - loss: 0.9160 - val_accuracy: 0.6265 - val_loss: 1.0180\n",
            "Epoch 59/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.6569 - loss: 0.9154 - val_accuracy: 0.6237 - val_loss: 1.0259\n",
            "Epoch 60/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6695 - loss: 0.8907 - val_accuracy: 0.6218 - val_loss: 1.0141\n",
            "Epoch 61/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6715 - loss: 0.8917 - val_accuracy: 0.6257 - val_loss: 1.0228\n",
            "Epoch 62/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.6681 - loss: 0.8916 - val_accuracy: 0.6281 - val_loss: 1.0127\n",
            "Epoch 63/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.6693 - loss: 0.8957 - val_accuracy: 0.6237 - val_loss: 1.0200\n",
            "Epoch 64/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 0.6673 - loss: 0.8912 - val_accuracy: 0.6282 - val_loss: 1.0111\n",
            "Epoch 65/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6754 - loss: 0.8775 - val_accuracy: 0.6242 - val_loss: 1.0163\n",
            "Epoch 66/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6779 - loss: 0.8775 - val_accuracy: 0.6210 - val_loss: 1.0207\n",
            "Epoch 67/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 50ms/step - accuracy: 0.6746 - loss: 0.8766 - val_accuracy: 0.6310 - val_loss: 1.0161\n",
            "Epoch 68/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6797 - loss: 0.8692 - val_accuracy: 0.6329 - val_loss: 1.0113\n",
            "Epoch 69/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6844 - loss: 0.8558 - val_accuracy: 0.6254 - val_loss: 1.0138\n",
            "Epoch 70/70\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6796 - loss: 0.8687 - val_accuracy: 0.6356 - val_loss: 1.0139\n"
          ]
        }
      ],
      "source": [
        "# Training the model using the training data (x_train, y_train)\n",
        "# Validating the model using the validation data (x_test, y_test)\n",
        "history = model.fit(x=x_train, y=y_train, batch_size=128, epochs=70, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lPNhl-_XDNsl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPNhl-_XDNsl",
        "outputId": "708539cc-99cd-4f66-d978-97dc1f706bd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Training Loss: 0.8635\n",
            "Final Training Accuracy: 0.6810\n",
            "Final Validation Loss: 1.0139\n",
            "Final Validation Accuracy: 0.6356\n"
          ]
        }
      ],
      "source": [
        "# Get the training history dictionary\n",
        "history_dict = history.history\n",
        "\n",
        "# Print final values after last epoch\n",
        "final_train_loss = history_dict['loss'][-1]\n",
        "final_train_acc = history_dict['accuracy'][-1]\n",
        "\n",
        "final_val_loss = history_dict['val_loss'][-1]\n",
        "final_val_acc = history_dict['val_accuracy'][-1]\n",
        "\n",
        "print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
        "print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
        "print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7eecd60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7eecd60",
        "outputId": "8ff35c6b-35d5-41a9-c6fb-374c1a98a2cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Saving the model weights to an HDF5 file\n",
        "model.save(\"emotion-detection.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aec3a442",
      "metadata": {
        "id": "aec3a442"
      },
      "source": [
        "## Loading trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MN02ZTAbOQTI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN02ZTAbOQTI",
        "outputId": "88e8f323-f5e5-45ed-91ae-c905ff47062b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jqeoRBCfboM1",
      "metadata": {
        "id": "jqeoRBCfboM1"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"emotion-detection.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gzR976wKbpPl",
      "metadata": {
        "id": "gzR976wKbpPl"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"emotion-detection.h5\")\n",
        "\n",
        "# Optional (safe way)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Now evaluate/train\n",
        "model.evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "028549ae",
      "metadata": {
        "id": "028549ae"
      },
      "outputs": [],
      "source": [
        "label=['angry','disgust','fear','happy','neutral','sad','surprise']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09cd7fbf",
      "metadata": {
        "id": "09cd7fbf"
      },
      "outputs": [],
      "source": [
        "# Defining function to preprocess of a single image\n",
        "def ef(image):\n",
        "    img = load_img(image, grayscale=True)\n",
        "    feature = np.array(img)\n",
        "    feature = feature.reshape(1, 48, 48, 1)  # Reshaping in place\n",
        "    return feature / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c10f34",
      "metadata": {
        "id": "76c10f34",
        "outputId": "466c9b2d-e02a-4e0b-b795-2b4a51e5b4a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original image is angry\n",
            "1/1 [==============================] - 0s 271ms/step\n",
            "Model prediction is angry\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Loading image, predicting, and displaying the image\n",
        "image_path = 'images/train/angry/22.jpg'\n",
        "print(\"Original image is angry\")\n",
        "img = ef(image_path)\n",
        "pred = model.predict(img)\n",
        "pred_label = label[pred.argmax()]\n",
        "print(\"Model prediction is\", pred_label)\n",
        "plt.imshow(img.reshape(48, 48), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5bcbaa2",
      "metadata": {
        "id": "d5bcbaa2"
      },
      "source": [
        "# Code for Real-Time Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407dcdf0",
      "metadata": {
        "id": "407dcdf0"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for realtime prediction\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "model = keras.models.load_model(\"emotion-detection.h5\")\n",
        "# Loading Haar cascade for face detection\n",
        "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "face_cascade = cv2.CascadeClassifier(haar_file)\n",
        "\n",
        "# Defining a function to extract features from an image\n",
        "def extract_features(image):\n",
        "    feature = np.array(image)\n",
        "    feature = feature.reshape(1, 48, 48, 1)\n",
        "    return feature / 255.0\n",
        "\n",
        "# Open webcam\n",
        "webcam = cv2.VideoCapture(0)\n",
        "\n",
        "# Defining dictionary for mapping numerical labels to emotions\n",
        "labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
        "\n",
        "# Continuous looping for capturing and processing webcam frames\n",
        "while True:\n",
        "    # Reading frame from webcam\n",
        "    i, im = webcam.read()\n",
        "    # Converting frame to grayscale\n",
        "    gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "    # Detecting faces in the grayscale frame\n",
        "    faces = face_cascade.detectMultiScale(im, 1.3, 5)\n",
        "    try:\n",
        "        # Iterating over detected faces\n",
        "        for (p, q, r, s) in faces:\n",
        "            # Croping face region from the grayscale frame\n",
        "            image = gray[q:q+s, p:p+r]\n",
        "            # Drawing rectangle around the detected face\n",
        "            cv2.rectangle(im, (p, q), (p+r, q+s), (255, 0, 0), 2)\n",
        "            # Resize the cropped face region to match model input size\n",
        "            image = cv2.resize(image, (48, 48))\n",
        "            # Extracting features from the resized face image\n",
        "            img = extract_features(image)\n",
        "            # Making prediction using the loaded model\n",
        "            pred = model.predict(img)\n",
        "            # Getting the predicted emotion label\n",
        "            prediction_label = labels[pred.argmax()]\n",
        "            # Displaying the predicted emotion label on the frame\n",
        "            cv2.putText(im, '%s' %(prediction_label), (p-10, q-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 0, 255))\n",
        "        # Displaying the frame with predicted emotion labels\n",
        "        cv2.imshow(\"Output\", im)\n",
        "        # Waitting for key press and close the window if 'Esc' key is pressed\n",
        "        cv2.waitKey(27)\n",
        "    except cv2.error:\n",
        "        pass  # Skip processing if an error occurs (e.g., no face detected)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
